import asyncio
import io
import traceback
import logging
from docx import Document

logger = logging.getLogger(__name__)

# Import all our services
logger.info("Importing services...")
try:
    logger.info("Importing protocol_parser...")
    from services import protocol_parser
    logger.info("‚úÖ protocol_parser imported")
    
    logger.info("Importing drug_normalizer...")
    from services import drug_normalizer
    logger.info("‚úÖ drug_normalizer imported")
    
    logger.info("Importing regulatory_checker...")
    from services import regulatory_checker
    logger.info("‚úÖ regulatory_checker imported")
    
    logger.info("Importing pubmed_client...")
    from services import pubmed_client
    logger.info("‚úÖ pubmed_client imported")
    
    logger.info("Importing ai_analyzer...")
    from services import ai_analyzer
    logger.info("‚úÖ ai_analyzer imported")
    
except ImportError as e:
    logger.error(f"‚ùå Import error: {e}")
    logger.error(f"Traceback: {traceback.format_exc()}")
    traceback.print_exc()
    raise

# Instantiate clients/services
logger.info("Creating PubMed client...")
pubmed = pubmed_client.PubMedClient()
logger.info("‚úÖ PubMed client created")

async def generate_document_summary(full_text: str) -> str:
    """Uses Gemini to generate a brief summary of the entire document."""
    logger.info("üìù Starting document summary generation...")
    if not full_text:
        logger.warning("‚ùå Empty text for summary generation")
        return "–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø—É—Å—Ç."

    logger.info(f"üìù Generating summary for text of length: {len(full_text)}")
    
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ç–æ–∫–æ–ª –∏ –Ω–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).

–£–∫–∞–∂–∏:
- –û—Å–Ω–æ–≤–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ –∏–ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
- –¶–µ–ª–µ–≤—É—é –≥—Ä—É–ø–ø—É –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤
- –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ª–µ—á–µ–Ω–∏—é

–¢–µ–∫—Å—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª–∞:
---
{full_text[:15000]}
---

–†–µ–∑—é–º–µ:"""
    
    try:
        logger.info("ü§ñ Sending summary request to Gemini...")
        import google.generativeai as genai
        model = genai.GenerativeModel(
            model_name="gemini-2.5-flash",
            generation_config={"temperature": 0.2, "max_output_tokens": 500}
        )
        response = await model.generate_content_async(prompt)
        summary = response.text.strip()
        logger.info(f"‚úÖ Summary generated: {len(summary)} chars")
        return summary
    except Exception as e:
        logger.error(f"‚ùå Error generating document summary: {e}")
        return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞."

async def process_single_drug(drug_info: dict, document_context: str = ""):
    """
    Processes a single drug through the complete analysis pipeline.
    Enhanced to use existing INN from extraction and better context.
    """
    source_name = drug_info.get("drug_name_source")
    existing_inn = drug_info.get("inn_name", "")
    
    if not source_name:
        logger.warning(f"‚ùå Skipping drug with no source name: {drug_info}")
        return None

    logger.info(f"üíä Processing drug: {source_name}")
    
    try:
        logger.info(f"üîÑ Normalizing drug name: {source_name}")
        # Use existing INN if provided by extraction
        if existing_inn and existing_inn.lower() not in ['unknown', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω']:
            normalization_result = await drug_normalizer.normalize_drug_with_existing_inn(source_name, existing_inn)
        else:
            normalization_result = await drug_normalizer.normalize_drug(source_name)
        
        inn_name = normalization_result.get("inn_name")
        logger.info(f"‚úÖ Normalization result: {normalization_result}")
    except Exception as e:
        logger.error(f"‚ùå Normalization failed for {source_name}: {e}")
        normalization_result = {"inn_name": None, "source": "Error", "confidence": "none"}
        inn_name = None

    if not inn_name:
        logger.warning(f"‚ö†Ô∏è No INN found for {source_name}, returning partial data")
        return {
            "source_data": drug_info,
            "normalization": normalization_result,
            "regulatory_checks": {"regulatory_checks": {}, "dosage_check": {}},
            "pubmed_articles": [],
            "ai_analysis": {
                "ud_ai_grade": "Unknown",
                "ud_ai_justification": "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ú–ù–ù –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞",
                "ai_summary_note": "–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞"
            }
        }

    logger.info(f"‚úÖ Found INN: {inn_name}, proceeding with full analysis")
    
    try:
        logger.info(f"üîç Starting regulatory and PubMed checks for {inn_name}")
        
        # Enhanced context for PubMed search
        search_context = drug_info.get("context_indication", "")
        if not search_context and document_context:
            # Use document context if drug context is empty
            search_context = document_context[:200]  # First 200 chars of document
        
        # Run regulatory and PubMed checks in parallel
        regulatory_task = regulatory_checker.check_all_regulators(
            inn_name=inn_name,
            source_dosage=drug_info.get("dosage_source", "")
        )
        
        pubmed_task = pubmed.search_articles(
            inn_name=inn_name,
            brand_name=source_name,
            disease_context=search_context
        )

        logger.info("üîÑ Running regulatory and PubMed tasks in parallel...")
        regulatory_results, pubmed_articles = await asyncio.gather(
            regulatory_task, 
            pubmed_task, 
            return_exceptions=True
        )
        logger.info("‚úÖ Parallel tasks completed")
        
        # Handle exceptions in results
        if isinstance(regulatory_results, Exception):
            logger.error(f"‚ùå Regulatory check failed: {regulatory_results}")
            regulatory_results = {"regulatory_checks": {}, "dosage_check": {}}
            
        if isinstance(pubmed_articles, Exception):
            logger.error(f"‚ùå PubMed search failed: {pubmed_articles}")
            pubmed_articles = []
            
    except Exception as e:
        logger.error(f"‚ùå Error in parallel tasks: {e}")
        regulatory_results = {"regulatory_checks": {}, "dosage_check": {}}
        pubmed_articles = []

    # Build full drug data
    full_drug_data = {
        "source_data": drug_info,
        "normalization": normalization_result,
        "regulatory_checks": regulatory_results,
        "pubmed_articles": pubmed_articles,
    }

    try:
        logger.info(f"üß† Generating final analysis for {source_name}")
        final_analysis = await ai_analyzer.generate_ai_analysis(full_drug_data)
        logger.info(f"‚úÖ Analysis completed for {source_name}")
    except Exception as e:
        logger.error(f"‚ùå Analysis failed for {source_name}: {e}")
        final_analysis = {
            "ud_ai_grade": "Error",
            "ud_ai_justification": "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞",
            "ai_summary_note": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑"
        }
        
    full_drug_data["ai_analysis"] = final_analysis
    
    logger.info(f"‚úÖ Completed full analysis for: {source_name}")
    return full_drug_data

async def run_analysis_pipeline(file_content: bytes):
    """
    Enhanced main orchestrator using unified extraction approach like demo.
    """
    logger.info("üöÄ Starting enhanced analysis pipeline")
    logger.info(f"File content size: {len(file_content)} bytes")
    
    file_stream = io.BytesIO(file_content)
    
    try:
        logger.info("üìÑ Loading DOCX document...")
        document = Document(file_stream)
        logger.info("‚úÖ Document loaded successfully")
    except Exception as e:
        logger.error(f"‚ùå Error loading document: {e}")
        return {"error": f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}"}

    logger.info("üìÑ Extracting text from document...")
    all_text = []
    
    # Extract from paragraphs
    for paragraph in document.paragraphs:
        if paragraph.text.strip():
            all_text.append(paragraph.text)
    
    # Extract from tables
    for table in document.tables:
        for row in table.rows:
            for cell in row.cells:
                if cell.text.strip():
                    all_text.append(cell.text)
    
    full_text = "\n".join(all_text)
    logger.info(f"üìÑ Extracted text length: {len(full_text)} chars")

    if not full_text.strip():
        logger.error("‚ùå Document is empty")
        return {"error": "–î–æ–∫—É–º–µ–Ω—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞."}

    try:
        # Generate summary and extract drugs concurrently
        logger.info("üîÑ Starting summary and unified drug extraction")
        summary_task = generate_document_summary(full_text)
        extraction_task = protocol_parser.extract_drugs_from_text(full_text)

        logger.info("‚è≥ Waiting for summary and extraction tasks...")
        document_summary, extracted_drugs = await asyncio.gather(
            summary_task, 
            extraction_task,
            return_exceptions=True
        )
        
        # Handle exceptions
        if isinstance(document_summary, Exception):
            logger.error(f"‚ùå Summary generation failed: {document_summary}")
            document_summary = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ"
            
        if isinstance(extracted_drugs, Exception):
            logger.error(f"‚ùå Drug extraction failed: {extracted_drugs}")
            extracted_drugs = []
        
        logger.info(f"‚úÖ Summary and extraction completed. Found {len(extracted_drugs)} drugs")
        
    except Exception as e:
        logger.error(f"‚ùå Error in summary/extraction: {e}")
        return {"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}"}

    if not extracted_drugs:
        logger.warning("‚ö†Ô∏è No drugs found in document")
        return {
            "document_summary": document_summary,
            "analysis_results": [],
            "message": "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤."
        }

    logger.info(f"üîÑ Processing {len(extracted_drugs)} extracted drugs")
    for i, drug in enumerate(extracted_drugs):
        logger.info(f"  Drug {i+1}: {drug.get('drug_name_source', 'Unknown')} (INN: {drug.get('inn_name', 'Unknown')})")

    try:
        logger.info("üîÑ Starting parallel drug analysis...")
        # Pass document context to each drug processing
        analysis_tasks = [process_single_drug(drug, full_text[:1000]) for drug in extracted_drugs]
        analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
        logger.info("‚úÖ All drug analysis tasks completed")
        
        # Filter out exceptions and None results
        valid_results = []
        for i, result in enumerate(analysis_results):
            if isinstance(result, Exception):
                logger.error(f"‚ùå Analysis failed for drug {i}: {result}")
            elif result is not None:
                valid_results.append(result)
                logger.info(f"‚úÖ Drug {i+1} analysis successful")
        
        logger.info(f"‚úÖ Pipeline completed. {len(valid_results)} drugs analyzed successfully")
        
        return {
            "document_summary": document_summary,
            "analysis_results": valid_results
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error in drug analysis: {e}")
        logger.error(traceback.format_exc())
        return {"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤: {e}"}